{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import RepeatVector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "tf.random.set_seed(8) # fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global param\n",
    "Y_COL = 1\n",
    "FEATURE_NUMBER = 2\n",
    "DATA_LOCATION = \"../Data/20230625-Data_for_ML\"\n",
    "DATA_FILE_NAME = \"Data_for_ML_TT-DB0\"\n",
    "MODEL_LOCATION = \"../Model\"\n",
    "SAVE_MODEL_LOCATION = f\"{MODEL_LOCATION}/{DATA_FILE_NAME}\"\n",
    "# Hyperparameter\n",
    "TRAIN_TEST_RATIO = 0.8\n",
    "TIME_STEP = 10\n",
    "MODEL_TYPE = \"CNN_SSO_9\"\n",
    "EPOCH_SIZE = 1000\n",
    "BATCH_SIZE = 2\n",
    "MODEL_JSON = {}\n",
    "MIN_DELTA = 0.0001\n",
    "PATIENCE = 15\n",
    "# X = [11, 2, 1, 1, 17, 1, 1, 1, 36]\n",
    "X = [41,2,1,1,16,1,1,1,864]\n",
    "# X = [8,2,1,1,5,1,1,1,931]\n",
    "# X = [15,2,1,1,51,1,1,1,122]\n",
    "# X = [7,2,1,1,98,1,1,1,55]\n",
    "# X = [4,2,1,1,90,1,1,1,328]\n",
    "CNN_LSTM = [2,2,1,248,257,51,8]\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    min_delta=MIN_DELTA,\n",
    "    patience=PATIENCE,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "def to_string():\n",
    "    print_list = [\n",
    "        f\"Time step: {TIME_STEP}\",\n",
    "        f\"Ratio: {TRAIN_TEST_RATIO}\",\n",
    "        f\"Model type: {MODEL_TYPE}\",\n",
    "        f\"Epoch size: {EPOCH_SIZE}\",\n",
    "        f\"Batch size: {BATCH_SIZE}\",\n",
    "        f\"Early stop patience: {PATIENCE}, min delta: {MIN_DELTA}\",\n",
    "        f\"Model json: {MODEL_JSON}\",\n",
    "    ]\n",
    "    return \"\\n\".join(print_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataframe = pd.read_excel(\n",
    "    f\"{DATA_LOCATION}/{DATA_FILE_NAME}.xlsx\",\n",
    "    sheet_name=\"Sheet1\",\n",
    "    header=1,\n",
    "    usecols=[0, 2],\n",
    ")\n",
    "dataset = dataframe.values\n",
    "# # Preprocessing\n",
    "for i in range(dataset.shape[0]):\n",
    "    dataset[i][0] = int(dataset[i][0].replace(\"TT\", \"\").replace(\"'\", \"\"))\n",
    "# dataset[np.isnan(dataset)] = 0\n",
    "dataset = dataset.astype(\"float64\")\n",
    "new_dataset = np.array(np.zeros((1, FEATURE_NUMBER)))\n",
    "j = 0\n",
    "mean = np.mean(dataset[:, Y_COL])\n",
    "print(\"mean\", mean)\n",
    "standard_deviation = np.std(dataset[:, Y_COL])\n",
    "print(\"std\", standard_deviation)\n",
    "\n",
    "bins_size = int(1 + 3.3 * np.log10(len(dataset)))\n",
    "counts, bin_dataset = np.histogram(dataset[:, Y_COL], bins=bins_size, density=True)\n",
    "counts_number, bin_dataset = np.histogram(dataset[:, Y_COL], bins=bins_size)\n",
    "rel_freq = counts_number / len(dataset)\n",
    "print(\"counts\", counts)\n",
    "print(\"counts_number\", counts_number)\n",
    "print(\"bin_dataset\", bin_dataset)\n",
    "plt.clf()\n",
    "plt.plot(bin_dataset[1:], rel_freq, label=\"PDF of LC\")\n",
    "plt.xlabel(\"Number of data\")\n",
    "plt.ylabel(\"Percentage %\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Take 95% of the data and by interval\n",
    "# for i in range(dataset.shape[0]):\n",
    "#     #     if dataset[i][0] < 50 and dataset[i][0] < 90:\n",
    "#     if (\n",
    "#         dataset[i][Y_COL] > mean - 2 * standard_deviation\n",
    "#         and dataset[i][Y_COL] < mean + 2 * standard_deviation\n",
    "#     ):\n",
    "#         new_dataset = np.append(new_dataset, [dataset[i]], axis=0)\n",
    "#         j += 1\n",
    "# dataset = new_dataset[1:, :]\n",
    "\n",
    "# cumulative dataset\n",
    "# for i in range(1, dataset.shape[0]):\n",
    "#     dataset[i, Y_COL] = dataset[i - 1, Y_COL] + dataset[i, Y_COL]\n",
    "    # dataset[i] = dataset[i-1] + dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(dataset[:,Y_COL], color=\"blue\", label=\"LC\")\n",
    "plt.xlabel(\"Dataset number\")\n",
    "plt.ylabel(\"LC\")\n",
    "leg = plt.legend()\n",
    "plt.twinx()\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.plot(dataset[:,0], color=\"red\", label=\"Temperature\")\n",
    "leg = plt.legend()\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * TRAIN_TEST_RATIO)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(f\"Total : {len(dataset)}, Train: {len(train)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca to reduce features\n",
    "# pca = PCA(n_components = 2)\n",
    "# FEATURE_NUMBER = 2\n",
    "# train_x = pca.fit_transform(train)\n",
    "# test_x = pca.transform(test)\n",
    "# # normalize x\n",
    "# train_x = scaler.fit_transform(train_x)\n",
    "# test_x = scaler.transform(test_x)\n",
    "# print(train_x.shape)\n",
    "# print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1, y_col=Y_COL):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), :]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, y_col])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "# train_x, train_y = create_dataset(train, TIME_STEP)\n",
    "# test_x, test_y = create_dataset(test, TIME_STEP)\n",
    "train_x = scaler.fit_transform(train)\n",
    "test_x = scaler.transform(test)\n",
    "train_x, _ = create_dataset(train_x, TIME_STEP)\n",
    "test_x, _ = create_dataset(test_x, TIME_STEP)\n",
    "_, train_y = create_dataset(train, TIME_STEP,Y_COL)\n",
    "_, test_y = create_dataset(test, TIME_STEP,Y_COL)\n",
    "# normalize y \n",
    "# train_y = train_y.reshape(-1,1)\n",
    "# test_y = test_y.reshape(-1,1)\n",
    "train_y = scaler.fit_transform(train_y.reshape(-1,1))\n",
    "test_y = scaler.transform(test_y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], TIME_STEP, FEATURE_NUMBER))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], TIME_STEP, FEATURE_NUMBER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model = Sequential()\n",
    "if MODEL_TYPE == \"LSTM\":\n",
    "    model.add(LSTM(32, input_shape=(TIME_STEP, FEATURE_NUMBER), return_sequences=True))\n",
    "    model.add(LSTM(32, input_shape=(TIME_STEP, FEATURE_NUMBER)))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"CNN\":\n",
    "    model.add(\n",
    "        Conv1D(32, (2), activation=\"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER))\n",
    "    )\n",
    "    model.add(Conv1D(16, (2), activation=\"relu\"))\n",
    "    model.add(Conv1D(8, (1), activation=\"relu\"))\n",
    "    model.add(MaxPooling1D((2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"CNN_SSO_9\":\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            X[0],\n",
    "            kernel_size=(int(X[1])),\n",
    "            strides=(int(X[2])),\n",
    "            input_shape=(TIME_STEP, FEATURE_NUMBER),\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"uniform\",\n",
    "        )\n",
    "    )\n",
    "    model.add(MaxPooling1D(pool_size=(int(X[3]))))\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            X[4],\n",
    "            (int(X[5])),\n",
    "            strides=(int(X[6])),\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"uniform\",\n",
    "        )\n",
    "    )\n",
    "    # X[12], X[13] = 2, 2\n",
    "    model.add(MaxPooling1D(pool_size=(int(X[7]))))\n",
    "    # X[14], X[15] = 100, 10\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(X[8], activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"CNN_SSO\":\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            X[0],\n",
    "            kernel_size=(int(X[1])),\n",
    "            strides=(int(X[3])),\n",
    "            input_shape=(TIME_STEP, FEATURE_NUMBER),\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"uniform\",\n",
    "        )\n",
    "    )\n",
    "    model.add(MaxPooling1D(pool_size=(int(X[5]))))\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            X[7],\n",
    "            (int(X[8])),\n",
    "            strides=(int(X[10])),\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\",\n",
    "            kernel_initializer=\"uniform\",\n",
    "        )\n",
    "    )\n",
    "    # X[12], X[13] = 2, 2\n",
    "    model.add(MaxPooling1D(pool_size=(int(X[12]))))\n",
    "    # X[14], X[15] = 100, 10\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(X[14], activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"CNN_LSTM\":\n",
    "    model.add(\n",
    "        Conv1D(6, (2), activation=\"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER))\n",
    "    )\n",
    "    model.add(LSTM(300, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER)))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"CNN_LSTM_DNN_SSO\":\n",
    "    model.add(\n",
    "        Conv1D(CNN_LSTM[0], kernel_size=CNN_LSTM[1], strides=CNN_LSTM[2], activation=\"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER))\n",
    "    )\n",
    "    model.add(LSTM(CNN_LSTM[3], \"relu\", return_sequences=True))\n",
    "    model.add(LSTM(CNN_LSTM[4], \"relu\"))\n",
    "    model.add(Dense(CNN_LSTM[5]))\n",
    "    model.add(Dense(CNN_LSTM[6]))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"CNN_LSTM_DNN\":\n",
    "    model.add(\n",
    "        Conv1D(8, (2), activation=\"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER))\n",
    "    )\n",
    "    model.add(LSTM(32, \"relu\", return_sequences=True))\n",
    "    model.add(LSTM(32, \"relu\"))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"CNN_LSTM_DNN2\":\n",
    "    model.add(\n",
    "        Conv1D(128, (2), activation=\"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER))\n",
    "    )\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            200, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER), return_sequences=True\n",
    "        )\n",
    "    )\n",
    "    model.add(LSTM(100, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"LSTM_DNN\":\n",
    "    model.add(\n",
    "        LSTM(32, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER), return_sequences=True)\n",
    "    )\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(\n",
    "        LSTM(32, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER), return_sequences=True)\n",
    "    )\n",
    "    model.add(LSTM(32, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"LSTM_DNN2\":\n",
    "    model.add(LSTM(300, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"LSTM_DNN3\":\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            300, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER), return_sequences=True\n",
    "        )\n",
    "    )\n",
    "    model.add(LSTM(300, \"relu\"))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"LSTM_DNN4\":\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            300, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER), return_sequences=True\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(300, \"relu\"))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "elif MODEL_TYPE == \"LSTM_DNN5\":\n",
    "    model.add(LSTM(300, \"relu\", input_shape=(TIME_STEP, FEATURE_NUMBER)))\n",
    "    model.add(Dense(1))\n",
    "model.summary()\n",
    "MODEL_JSON = model.to_json()\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=EPOCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=2,\n",
    "    callbacks=[EARLY_STOPPING, tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "version = 0\n",
    "if not os.path.exists(SAVE_MODEL_LOCATION):\n",
    "    os.makedirs(SAVE_MODEL_LOCATION)\n",
    "for dir in os.scandir(f\"{SAVE_MODEL_LOCATION}/\"):\n",
    "    if dir.is_dir():\n",
    "        version = int(dir.name) if int(dir.name) > version else version\n",
    "version += 1\n",
    "model.save(f\"{SAVE_MODEL_LOCATION}/{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "score = model.evaluate(test_x, test_y,batch_size=20, verbose=0)  # return MAPE\n",
    "print(score)\n",
    "train_predict = model.predict(train_x)\n",
    "test_predict = model.predict(test_x)\n",
    "# invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "train_true = scaler.inverse_transform(train_y).reshape(1, -1)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "test_true = scaler.inverse_transform(test_y).reshape(1, -1)\n",
    "\n",
    "# scaled\n",
    "# train_predict = train_predict\n",
    "# train_true = np.reshape(train_y,(1,-1))\n",
    "# test_predict = test_predict\n",
    "# test_true = np.reshape(test_y,(1,-1))\n",
    "\n",
    "# calculate mean absolute error (MAE)\n",
    "train_mae = mean_absolute_error(train_true[0], train_predict[:, 0])\n",
    "test_mae = mean_absolute_error(test_true[0], test_predict[:, 0])\n",
    "# calculate mean absolute percentage error (MAPE)\n",
    "train_mape = mean_absolute_percentage_error(train_true[0], train_predict[:, 0])\n",
    "test_mape = mean_absolute_percentage_error(test_true[0], test_predict[:, 0])\n",
    "# calculate mean squared error (MSE)\n",
    "train_mse = mean_squared_error(train_true[0], train_predict[:, 0])\n",
    "test_mse = mean_squared_error(test_true[0], test_predict[:, 0])\n",
    "# calculate root mean squared error (RMSE)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "# calculate root mean squared percentage error (RMSPE)\n",
    "train_rmspe = np.sqrt(\n",
    "    np.mean(\n",
    "        np.square(((train_true[0] - train_predict[:, 0]) / train_true[0])),\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "test_rmspe = np.sqrt(\n",
    "    np.mean(\n",
    "        np.square(((test_true[0] - test_predict[:, 0]) / test_true[0])),\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "train_mse_output = \"Train MSE Score: %.10f MSE\" % train_mse\n",
    "test_mse_output = \"Test MSE Score: %.10f MSE\" % test_mse\n",
    "train_mae_output = \"Train MAE Score: %.10f MAE\" % train_mae\n",
    "test_mae_output = \"Test MAE Score: %.10f MAE\" % test_mae\n",
    "train_mape_output = \"Train MAPE Score: %.4f MAPE\" % train_mape\n",
    "test_mape_output = \"Test MAPE Score: %.4f MAPE\" % test_mape\n",
    "train_rmse_output = \"Train RMSE Score: %.10f RMSE\" % train_rmse\n",
    "test_rmse_output = \"Test RMSE Score: %.10f RMSE\" % test_rmse\n",
    "train_rmspe_output = \"Train RMSPE: %.4f RMSPE\" % train_rmspe\n",
    "test_rmspe_output = \"Test RMSPE: %.4f RMSPE\" % test_rmspe\n",
    "print(train_mse_output)\n",
    "print(test_mse_output)\n",
    "print(train_mae_output)\n",
    "print(test_mae_output)\n",
    "print(train_mape_output)\n",
    "print(test_mape_output)\n",
    "print(train_rmse_output)\n",
    "print(test_rmse_output)\n",
    "print(train_rmspe_output)\n",
    "print(test_rmspe_output)\n",
    "\n",
    "rmse_file = open(f\"./{SAVE_MODEL_LOCATION}/{version}/info.md\", \"w\")\n",
    "rmse_file.write(\n",
    "    train_mse_output\n",
    "    + \"\\n\"\n",
    "    + test_mse_output\n",
    "    + \"\\n\"\n",
    "    + train_mae_output\n",
    "    + \"\\n\"\n",
    "    + test_mae_output\n",
    "    + \"\\n\"\n",
    "    + train_mape_output\n",
    "    + \"\\n\"\n",
    "    + test_mape_output\n",
    "    + \"\\n\"\n",
    "    + train_rmse_output\n",
    "    + \"\\n\"\n",
    "    + test_rmse_output\n",
    "    + \"\\n\"\n",
    "    + train_rmspe_output\n",
    "    + \"\\n\"\n",
    "    + test_rmspe_output\n",
    ")\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset[:, Y_COL]).reshape(-1, 1)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[TIME_STEP : len(train_predict) + TIME_STEP, :] = train_predict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset[:, Y_COL]).reshape(-1, 1)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[\n",
    "    len(train_predict) + (TIME_STEP * 2) + 1 : len(dataset) - 1, :\n",
    "] = test_predict\n",
    "\n",
    "true_values = (dataset)[:, Y_COL].reshape(-1, 1)\n",
    "# plot baseline and predictions\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.rcParams.update({\"font.size\": 10})\n",
    "plt.xlabel(\"Dataset number\")\n",
    "plt.ylabel(\"LC\")\n",
    "plt.plot(true_values, label=\"True values\")\n",
    "# plt.plot(test_true.reshape(-1,1), label=\"Test true\")\n",
    "plt.plot(trainPredictPlot, label=\"Train prediction\")\n",
    "plt.plot(testPredictPlot, label=\"Test prediction\")\n",
    "leg = plt.legend()\n",
    "\n",
    "plt.savefig(f\"{SAVE_MODEL_LOCATION}/{version}/predict_normal.png\")\n",
    "plt.show()\n",
    "# for i in range(1, train_predict):\n",
    "#     train_predict[i] = train_predict[i - 1] + train_predict[i]\n",
    "# for i in range(1, train_true):\n",
    "#     train_true[i] = train_true[i - 1] + train_true[i]\n",
    "# for i in range(1, test_predict):\n",
    "#     test_predict[i] = test_predict[i - 1] + test_predict[i]\n",
    "# for i in range(1, test_true):\n",
    "#     test_true[i] = test_true[i - 1] + test_true[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = model.predict(train_x)\n",
    "test_predict = model.predict(test_x)\n",
    "# invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "train_true = scaler.inverse_transform(train_y).reshape(1, -1)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "test_true = scaler.inverse_transform(test_y).reshape(1, -1)\n",
    "\n",
    "true_values = np.cumsum((dataset)[:,Y_COL].reshape(-1,1))\n",
    "\n",
    "train_true = np.cumsum(train_true).reshape(1, -1)\n",
    "test_true = true_values[-len(test_true[0]):].reshape(1,-1)\n",
    "\n",
    "train_predict = np.cumsum(train_predict).reshape(-1, 1)\n",
    "test_predict = np.cumsum(np.append(test_true[0, 0], test_predict))[1:].reshape(-1, 1)\n",
    "# test_predict = np.cumsum(test_predict).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# calculate mean absolute error (MAE)\n",
    "train_mae = mean_absolute_error(train_true[0], train_predict[:, 0])\n",
    "test_mae = mean_absolute_error(test_true[0], test_predict[:, 0])\n",
    "# calculate mean absolute percentage error (MAPE)\n",
    "train_mape = mean_absolute_percentage_error(train_true[0], train_predict[:, 0])\n",
    "test_mape = mean_absolute_percentage_error(test_true[0], test_predict[:, 0])\n",
    "# calculate mean squared error (MSE)\n",
    "train_mse = mean_squared_error(train_true[0], train_predict[:, 0])\n",
    "test_mse = mean_squared_error(test_true[0], test_predict[:, 0])\n",
    "# calculate root mean squared error (RMSE)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "# calculate root mean squared percentage error (RMSPE)\n",
    "train_rmspe = np.sqrt(\n",
    "    np.mean(\n",
    "        np.square(((train_true[0] - train_predict[:, 0]) / train_true[0])),\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "test_rmspe = np.sqrt(\n",
    "    np.mean(\n",
    "        np.square(((test_true[0] - test_predict[:, 0]) / test_true[0])),\n",
    "        axis=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "train_mse_output = \"Train MSE Score: %.10f MSE\" % train_mse\n",
    "test_mse_output = \"Test MSE Score: %.10f MSE\" % test_mse\n",
    "train_mae_output = \"Train MAE Score: %.10f MAE\" % train_mae\n",
    "test_mae_output = \"Test MAE Score: %.10f MAE\" % test_mae\n",
    "train_mape_output = \"Train MAPE Score: %.4f MAPE\" % train_mape\n",
    "test_mape_output = \"Test MAPE Score: %.4f MAPE\" % test_mape\n",
    "train_rmse_output = \"Train RMSE Score: %.10f RMSE\" % train_rmse\n",
    "test_rmse_output = \"Test RMSE Score: %.10f RMSE\" % test_rmse\n",
    "train_rmspe_output = \"Train RMSPE: %.4f RMSPE\" % train_rmspe\n",
    "test_rmspe_output = \"Test RMSPE: %.4f RMSPE\" % test_rmspe\n",
    "print(train_mse_output)\n",
    "print(test_mse_output)\n",
    "print(train_mae_output)\n",
    "print(test_mae_output)\n",
    "print(train_mape_output)\n",
    "print(test_mape_output)\n",
    "print(train_rmse_output)\n",
    "print(test_rmse_output)\n",
    "print(train_rmspe_output)\n",
    "print(test_rmspe_output)\n",
    "\n",
    "rmse_file = open(f\"./{SAVE_MODEL_LOCATION}/{version}/info.md\", \"a\")\n",
    "rmse_file.write(\n",
    "    \"\\n\\nCumulative:\\n\"\n",
    "    + train_mse_output\n",
    "    + \"\\n\"\n",
    "    + test_mse_output\n",
    "    + \"\\n\"\n",
    "    + train_mae_output\n",
    "    + \"\\n\"\n",
    "    + test_mae_output\n",
    "    + \"\\n\"\n",
    "    + train_mape_output\n",
    "    + \"\\n\"\n",
    "    + test_mape_output\n",
    "    + \"\\n\"\n",
    "    + train_rmse_output\n",
    "    + \"\\n\"\n",
    "    + test_rmse_output\n",
    "    + \"\\n\"\n",
    "    + train_rmspe_output\n",
    "    + \"\\n\"\n",
    "    + test_rmspe_output\n",
    "    + \"\\n\\n\"\n",
    "    + to_string()\n",
    ")\n",
    "rmse_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset[:,Y_COL]).reshape(-1,1)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[TIME_STEP:len(train_predict)+TIME_STEP, :] = train_predict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset[:,Y_COL]).reshape(-1,1)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(train_predict)+(TIME_STEP*2)+1:len(dataset)-1, :] = test_predict\n",
    "\n",
    "true_values = np.cumsum((dataset)[:,Y_COL].reshape(-1,1))\n",
    "print(true_values.shape)\n",
    "print(trainPredictPlot.shape)\n",
    "# plot baseline and predictions\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.xlabel(\"Dataset number\")\n",
    "plt.ylabel(\"LC\")\n",
    "plt.plot(true_values, label=\"True values\")\n",
    "# plt.plot(test_true.reshape(-1,1), label=\"Test true\")\n",
    "plt.plot(trainPredictPlot, label=\"Train prediction\")\n",
    "plt.plot(testPredictPlot, label=\"Test prediction\")\n",
    "leg = plt.legend()\n",
    "\n",
    "plt.savefig(f\"{SAVE_MODEL_LOCATION}/{version}/predict_cumulative.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
